# Activit√© 2.1 : Analyse Sources + Mod√®le Cible

## üéØ Contexte d'Usage
Analyser rapidement des fichiers de donn√©es r√©elles et concevoir le mod√®le dimensionnel optimal en approche int√©gr√©e.

## ‚è±Ô∏è Temps Allou√©
20 minutes

## üë§ Votre R√¥le
Data Engineer senior sp√©cialis√© dans l'int√©gration de donn√©es √©nerg√©tiques

## üìä Ressource Fournie
Un [fichier CVS](https://github.com/abdelkrim-brahmi/atelier-llm-data-observatoire-pv/blob/main/data/raw/prod-region-annuelle-filiere.csv) de donn√©es de production √©lectrique en France (dont photovolta√Øques)

## üéØ Livrable Attendu
Mod√®le dimensionnel + mapping source-cible complet

## ü§ñ Prompt √† Copier-Coller (avec Upload de Fichiers)

```
Agis comme un Data Engineer senior sp√©cialis√© dans l'int√©gration de donn√©es √©nerg√©tiques.

CONTEXTE : J'ai une source de donn√©es de production √©lectrique, dont photovolta√Øques, fran√ßaises [UPLOAD VOTRE FICHIER CSV]. Je dois cr√©er un data warehouse Snowflake optimis√© pour alimenter notre observatoire strat√©gique.

MISSION INT√âGR√âE : En analysant ces sources, con√ßois en une seule d√©marche :

1. ANALYSE DES SOURCES
   - Structure et sch√©ma de chaque fichier
   - Qualit√© des donn√©es (compl√©tude, coh√©rence, doublons)
   - Volum√©trie estim√©e et fr√©quence de mise √† jour
   - Probl√®mes potentiels identifi√©s

2. MOD√àLE DIMENSIONNEL OPTIMAL
   - Sch√©ma en √©toile adapt√© aux analyses photovolta√Øques
   - Tables de dimension (temps, g√©ographie, technologie, etc.)
   - Table(s) de faits avec m√©triques cl√©s
   - Granularit√© optimale pour les cas d'usage business

3. MAPPING SOURCE-VERS-CIBLE
   - Correspondance champs sources ‚Üí mod√®le cible
   - R√®gles de transformation n√©cessaires
   - Gestion des valeurs manquantes et aberrantes
   - Cl√©s de jointure et relations

4. R√àGLES DE CALCUL KPI
   - Formules pour les 5 indicateurs prioritaires (P0)
   - Agr√©gations temporelles et g√©ographiques
   - Gestion de l'historique et des √©volutions

Sois pr√©cis sur les noms de colonnes et types de donn√©es Snowflake.
```

## ‚úÖ Crit√®res de Validation

- [ ] Mod√®le dimensionnel coh√©rent (1-2 faits + 3-4 dimensions)
- [ ] Mapping complet source ‚Üí cible document√©
- [ ] R√®gles de calcul des KPIs P0 d√©finies
- [ ] Types Snowflake appropri√©s (VARCHAR, NUMBER, DATE, etc.)

## üí° Conseils d'Optimisation

### Pr√©paration des Fichiers
```
Avant d'upload, v√©rifiez que votre fichier :
- Est au format CSV avec encodage UTF-8
- A des headers explicites en premi√®re ligne
- Fait moins de 100 MB (limite upload Copilot)
```

### Maximiser l'Efficacit√© du Prompt
```
Pour optimiser l'analyse, ajoutez apr√®s le prompt principal :
"Base ton analyse sur un √©chantillon repr√©sentatif si les fichiers sont volumineux.
Privil√©gie la coh√©rence du mod√®le dimensionnel plut√¥t que l'exhaustivit√© des d√©tails.
Propose des noms de tables/colonnes respectant les conventions Snowflake."
```

## üîß Prompts de Sp√©cialisation

### Focus sur la Qualit√© des Donn√©es
```
Approfondis l'analyse qualit√© en ajoutant :
- Profiling statistique des colonnes num√©riques cl√©s
- D√©tection d'outliers avec seuils recommand√©s pour le photovolta√Øque
- Analyse des patterns temporels (saisonnalit√©, tendances)
- R√®gles de validation m√©tier sp√©cifiques au secteur √©nerg√©tique
- Strat√©gies de nettoyage et d'enrichissement automatis√©
```

### Optimisation Performance Snowflake
```
Enrichis le mod√®le avec les optimisations Snowflake :
- Strat√©gie de clustering pour les tables de faits (par date + r√©gion)
- Partitioning optimal pour l'historique (annuel/mensuel)
- Recommandations de vues mat√©rialis√©es pour les agr√©gations courantes
- Estimation des co√ªts de stockage et compute sur 3 ans
- Configuration warehouse selon les patterns d'usage
```

### Extensibilit√© du Mod√®le
```
Ajoute une section √©volutivit√© :
- Comment int√©grer de nouvelles sources (√©olien, hydraulique) ?
- Strat√©gie de migration sans interruption (blue/green deployment)
- Gestion du versioning du mod√®le de donn√©es
- Plan de mont√©e en charge (passage de Go √† To de donn√©es)
- Architecture multi-environnements (dev/test/prod)
```

## üìã Structure de R√©ponse Attendue

Afin de nous aligner sur les √©tapes suivantes :
- **Exemple d'analyse de source** :  [datawarehouse_conception_pv](../../sample%20results/analyse_sources_resultat.md)


## üîç Points de Vigilance

### Qualit√© des Donn√©es
- **Coh√©rence inter-sources** : V√©rifier que les totaux r√©gionaux matchent
- **Valeurs aberrantes** : Puissances > 1000 MW (centrales vs toitures)  
- **Compl√©tude temporelle** : S√©ries continues sans trous
- **R√©f√©rentiels** : Codes r√©gion coh√©rents entre sources

### Performance Snowflake
- **Clustering keys** : DATE + REGION pour les patterns de requ√™te courants
- **Taille des warehouses** : XS pour dev, M pour prod selon volum√©trie
- **Co√ªts** : Attention aux full-scan sur historique complet

## üéØ Validation Finale

### Checklist Technique
- [ ] Le mod√®le supporte-t-il tous les KPIs identifi√©s en Phase 1 ?
- [ ] Les jointures sont-elles optimis√©es (cl√©s num√©riques) ?
- [ ] La granularit√© permet-elle les analyses temporelles fines ?
- [ ] Le mod√®le est-il extensible pour de nouvelles sources ?

### Test de Coh√©rence
```sql
-- V√©rification des volumes apr√®s transformation
SELECT 
    'Source' as niveau,
    COUNT(*) as nb_records
FROM staging_registre_installations
UNION ALL
SELECT 
    'Cible' as niveau,
    COUNT(DISTINCT installation_key) as nb_records  
FROM FAIT_PRODUCTION_PV
-- Le nombre doit √™tre coh√©rent (¬±5% acceptable pour nettoyage)
```

Cette structure garantit un mod√®le dimensionnel robuste et optimis√© en 20 minutes d'analyse intensive !
